{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. Please check the pdf file for more details.*\n",
    "\n",
    "In this exercise you will:\n",
    "\n",
    "- implement Q-learning on a few different control tasks from the OpenAI Gym benchmark suite\n",
    "- implement approximators for (potentially) infinite and continous MDP, including linear approximator and neural networks\n",
    "- explore some popular tricks in RL, including exploration/exploitability trade-off, experience replay.\n",
    "\n",
    "Please note that **YOU CANNOT USE ANY MACHINE LEARNING PACKAGE SUCH AS SKLEARN** for any homework, unless you are asked to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "# some basic imports\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']= 'True'\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "try:\n",
    "    import gym\n",
    "except:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install gym --user\n",
    "    import gym\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first get to know the gym benchmark\n",
    "\n",
    "The [Gym](http://gym.openai.com/) is a famous benchmark suite from OpenAI. It provides some handful environments to play with the agents/algorithms you design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\ntable {float:left}\n</style>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    " %%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first play with a simple environment which is discrete and finite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze Lake\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "The surface is described using a grid like the following:\n",
    "\n",
    "| | |\n",
    "|:---|:---|\n",
    "|SFFF| (S: starting point, safe)|\n",
    "|FHFH| (F: frozen surface, safe)|\n",
    "|FFFH| (H: hole, fall to your doom)|\n",
    "|HFFG| (G: goal, where the frisbee is located)|  \n",
    "\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the simple version of this problem below. In the following environment, the ice is not slippery, so you can always move in the direction you intend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(Right)\nSFF\u001b[41mF\u001b[0m\nFHFH\nFFFH\nHFFG\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "None"
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-17cddb4bc1d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "env.reset()\n",
    "display(env.render())\n",
    "max_steps = 100\n",
    "for step in range(max_steps):\n",
    "    # take a random action\n",
    "    action = env.action_space.sample()\n",
    "    clear_output(wait=True)\n",
    "    # take the action and observe the outcome state and reward\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    display(env.render())\n",
    "    time.sleep(1)\n",
    "    state = new_state\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the expected reward for such a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0',is_slippery=False)\n",
    "total_episode = 1000\n",
    "total_reward = 0\n",
    "total_steps = 0\n",
    "max_steps = 100\n",
    "\n",
    "for episode in range(total_episode):\n",
    "    env.reset()\n",
    "    for step in range(max_steps):\n",
    "        # take a random action\n",
    "        action = env.action_space.sample()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        state = new_state\n",
    "        if done:\n",
    "            if reward == 1.0:\n",
    "                print('In episode {}, we takes {} steps to succeed!'.format(episode, step + 1))\n",
    "                total_reward += reward\n",
    "                total_steps += step + 1\n",
    "            break\n",
    "env.close()\n",
    "print('The expected reward of each episode is {}, and the average steps to succeed is {}'\n",
    "      .format(total_reward / total_episode, total_steps / total_reward\n",
    "             ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, even for this simple version, the random policy obviously works terribly.\n",
    "\n",
    "Now let's try to crack this environment with reinforcement learning!\n",
    "\n",
    "Let's implement Q-learning algorithm with Monte Carlo methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's breifly review the bellman equation for Q-learning algorithm.\n",
    "The derivation is actually simple.\n",
    "\n",
    "Considering a specific action $a$ at each state, we have:\n",
    "\n",
    "$Q(s_t, a_t)=R_{t+1}+\\gamma Q(s_{t+1}, a_{t+1})$\n",
    "\n",
    "Also, in state $s_{t+1}$, we would like to choose the action with the max expected reward, that is:\n",
    "\n",
    "$a_{t+1}=argmax_a Q(s_{t+1}, a)$\n",
    "\n",
    "As a result:\n",
    "\n",
    "$Q(s_{t+1}, a_{t+1}) = max_aQ(s_{t+1}, a)$\n",
    "\n",
    "Thus, we get the updated value:\n",
    "\n",
    "$Q(s_t, a_t)=R_{t+1}+\\gamma max_aQ(s_{t+1}, a)$\n",
    "\n",
    "Recall that in Monte Carlo method, we only update the q-value with a small $\\alpha$, i.e.:\n",
    "\n",
    "$Q(s_t, a_t)=Q(s_t, a_t)+\\alpha\\delta_Q=Q(s_t, a_t)+\\alpha(R_{t+1}+\\gamma max_a(Q(s_{t+1}, a)) - Q(s_t, a_t))$\n",
    "\n",
    "which is exactly the updated equation we will use for Q-learning.\n",
    "\n",
    "Please first complete the *bellman_equation_update* function in *qTable.py* bellow to this formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement Q-learning for this problem. This is basically updating the *qtable* with the *bellman_equation_update*.\n",
    "\n",
    "### TODO: Please implement the QTable class in *qTable.py*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qTable import QTable\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "print('action size:',action_size)\n",
    "print('state size:', state_size)\n",
    "total_episode = 100\n",
    "gamma = 0.95 # discounting rate\n",
    "alpha = 0.9 # learning rate for Q-learning\n",
    "max_steps = 100\n",
    "\n",
    "qTable = QTable(state_size, action_size, alpha, gamma)\n",
    "qTable.train(env, total_episode, max_steps)\n",
    "print(qTable.qtable)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is no episode that successfully reaches the goal position! Why? Try to think about it yourself before looking at the answers in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two reasons. The first (and not so important) reason is that qTable.qtable is initialized to be all zeros. As a result, the agent would always take one action and cannot update its qtable as it can never reach the goal grid. To deal with the problem, you can initialize the qTable.qtable with random numbers. If you are lucky enough, you may get some successful episodes. But try to rerun the cell for multiple times, you will find that the agent still cannot reach the goal grid for most episode.\n",
    "\n",
    "The second (and important) reason is that at the beginning of the training process, the *qtable* is extremely noisy. If we rely on it to make decision for each step, then it is very likely that we cannot reach the goal position. Moreover, as we cannot get the reward, the *bellman_equation* cannot effectively update the *qtable*, and we may end up with no succesful episode.\n",
    "\n",
    "This is exactly the reason we employ $\\epsilon$-greedy strategy for the training process. Recall that $\\epsilon$-greedy strategy means that the agent, in every state, takes random actions with a $\\epsilon$ probability while takes the optimal action with a $1-\\epsilon$ probability. Actually, this is often called **exploration** in reinforcement learning. At the beginning of the training process, the agent should explore the environment (maybe aggressively) to discover actions that lead to high rewards: this is a key component of reinforcement learning. In this case, the $\\epsilon$ should be large. After some training iterations, the agent should be more clever to take good actions, and thus we can take a smaller $\\epsilon$ to decrease the exploration. This is often called **exploitation** in reinforcement learning, as we try to exploit the existing policy/qtable. The exploration and exploitation trade-off is often a key trade-off in reinforcement learning. Briefly speaking, in the beginning of training, the agent should focus more on the exploration to train a good policy; as training goes by, the agent should pay more and more attention on exploitability. In practice, we often use a exponential decay strategy to decay the exploration probability.\n",
    "\n",
    "### TODO: Add the epsilon-greedy strategy in the *QTable* class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qTable import QTable\n",
    "\n",
    "env = gym.make('FrozenLake-v0',is_slippery=False)\n",
    "\n",
    "total_episode = 2000\n",
    "\n",
    "gamma = 0.95                  # discounting rate\n",
    "alpha = 0.8                   # learning rate for Q-learning\n",
    "max_steps = 100\n",
    "\n",
    "# exploration parameters\n",
    "epsilon = 1.0                 # the initial exploration probability\n",
    "min_epsilon = 0.0             # the minimum exploration probability \n",
    "epsilon_decay = 0.9           # the exponential decay rate for exploration prob\n",
    "\n",
    "# Your code here: tune the epsilon, min_epsilon, epsilon_decay yourself to make the testing average reward of your\n",
    "# agent be higher than 0.8.\n",
    "# If may be helpful for you to print out the reward and steps for each episode\n",
    "# to understand whether you require more exploration or exploitation.\n",
    "# Basically, the higher the epsilon_decay, the more exploration you have in the beginning\n",
    "# of training.\n",
    "# It is very imporant for you to practice itself to get a sense of the exploration\n",
    "# and exploitation trade-off.\n",
    "# begin answer\n",
    "# end answer\n",
    "\n",
    "qTable = QTable(state_size, action_size, alpha, gamma,\n",
    "               init_epsilon=epsilon,\n",
    "               min_epsilon=min_epsilon,\n",
    "               epsilon_decay=epsilon_decay)\n",
    "qTable.train(env, total_episode, max_steps)\n",
    "print(qTable.qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(qTable.step_list))\n",
    "plt.title('The change of step with training', fontsize=13)\n",
    "plt.legend(['step number'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can now use the *qtable* for our MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0',is_slippery=False)\n",
    "\n",
    "total_episode = 1000\n",
    "\n",
    "qTable.eval(env, total_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also interesting to have a look at how the process works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0',is_slippery=False)\n",
    "state = env.reset()\n",
    "display(env.render())\n",
    "max_steps = 100\n",
    "# the table has been trained well; thus we don't need exploration\n",
    "qTable.set_epsilon(0)\n",
    "for step in range(max_steps):\n",
    "    # take a random action\n",
    "    action = qTable.take_action(state)\n",
    "    clear_output(wait=True)\n",
    "    # take the action and observe the outcome state and reward\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    display(env.render())\n",
    "    time.sleep(1)\n",
    "    state = new_state\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, in the determinstic environment, i.e. there is no slippery, the Q-learning learns the optimal strategy which can takes the minimal steps to the goal, which is desirable. Let's see how this works for slippery environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "total_episode = 10000\n",
    "\n",
    "gamma = 0.95                  # discounting rate\n",
    "alpha = 0.8                   # learning rate for Q-learning\n",
    "max_steps = 100\n",
    "\n",
    "# exploration parameters\n",
    "epsilon = 1.0                 # the initial exploration probability\n",
    "min_epsilon = 0.0             # the minimum exploration probability \n",
    "epsilon_decay = 0.9           # the exponential decay rate for exploration prob\n",
    "\n",
    "# Your code here: tune the epsilon, min_epsilon, epsilon_decay yourself to make the testing average reward of your\n",
    "# agent be higher than 0.7.\n",
    "# If may be helpful for you to print out the reward and steps for each episode\n",
    "# to understand whether you require more exploration or exploitation.\n",
    "# Basically, the higher the epsilon_decay, the more exploration you have in the beginning\n",
    "# of training.\n",
    "# It is very imporant for you to practice itself to get a sense of the exploration\n",
    "# and exploitation trade-off.\n",
    "\n",
    "# begin answer\n",
    "# end answer\n",
    "\n",
    "qTable = QTable(state_size, action_size, alpha, gamma,\n",
    "               init_epsilon=epsilon,\n",
    "               min_epsilon=min_epsilon,\n",
    "               epsilon_decay=epsilon_decay)\n",
    "qTable.train(env, total_episode, max_steps)\n",
    "print(qTable.qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(qTable.step_list))\n",
    "plt.title('The change of step with training (slippery)', fontsize=13)\n",
    "plt.legend(['step number'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episode = 1000\n",
    "\n",
    "qTable.eval(env, total_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the expected reward for this non-deterministic environment is lower than the reward for the previous one. However, Q-learning still works pretty good for this setting. Note that we may never get a perfect expected reward for this problem due to the random slippery. Also have a look at the demo below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "state = env.reset()\n",
    "display(env.render())\n",
    "max_steps = 100\n",
    "qTable.set_epsilon(0.0)\n",
    "for step in range(max_steps):\n",
    "    # take a random action\n",
    "    action = qTable.take_action(state)\n",
    "    clear_output(wait=True)\n",
    "    # take the action and observe the outcome state and reward\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    display(env.render())\n",
    "    time.sleep(1)\n",
    "    state = new_state\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's crack a little bit harder game. This environment is discrete but infinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Chain\n",
    "\n",
    "This game presents moves along a linear chain of states, with two actions:\n",
    "* 0. forward, which moves along the chain but returns no reward\n",
    "* 1. backward, which returns to the beginning and has a small reward\n",
    "\n",
    "The end of the chain, however, presents a large reward, and by moving 'forward' at the end of the chain this large reward can be repeated.\n",
    "\n",
    "At each action, there is a small probability that the agent 'slips' and the opposite transition is instead taken.\n",
    "\n",
    "The observed state is the current state in the chain (0 to n-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('NChain-v0',\n",
    "               n=5, # the length of the chain is 5\n",
    "               slip=0.2, # the slipping probability\n",
    "               small=2, # the small reward for backward\n",
    "               large=10 # the large reward for go forward\n",
    "              )\n",
    "state = env.reset()\n",
    "total_episode = 1000\n",
    "max_steps = 100\n",
    "total_reward = 0\n",
    "for episode in range(total_episode):\n",
    "    state = env.reset()\n",
    "    for step in range(max_steps):\n",
    "        # take a random action\n",
    "        # action = env.action_space.sample()\n",
    "        action = 0\n",
    "        # take the action and observe the outcome state and reward\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        # print(reward, total_reward)\n",
    "        state = new_state\n",
    "env.close()\n",
    "print('Average reward is {}'.format(total_reward / episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "It's actually interesting to try strategy of always going forward or backward. Have a try here, and explain such phenomena in your PDF writeup file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply Q-learning for this environment and see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qTable import QTable\n",
    "\n",
    "env = gym.make('NChain-v0',\n",
    "               n=5, # the length of the chain is 5\n",
    "               slip=0.2, # the slipping probability\n",
    "               small=2, # the small reward for backward\n",
    "               large=10 # the large reward for go forward\n",
    "              )\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "gamma = 0.95 # discounting rate\n",
    "alpha = 0.8  # learning rate for Q-learning\n",
    "\n",
    "# exploration parameters\n",
    "epsilon = 1                 # the initial exploration probability\n",
    "min_epsilon = 0            # the minimum exploration probability \n",
    "epsilon_decay = 0.8      # the exponential decay rate for exploration prob\n",
    "\n",
    "qTable = QTable(state_size, action_size, alpha, gamma,\n",
    "               init_epsilon=epsilon,\n",
    "               min_epsilon=min_epsilon,\n",
    "               epsilon_decay=epsilon_decay)\n",
    "total_episode = 1000\n",
    "max_steps = 100\n",
    "qTable.train(env, total_episode, max_steps)\n",
    "print(qTable.qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(qTable.reward_list))\n",
    "plt.title('change of reward with training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 denotes going forward, and 1 denotes going backward. Please pay attention to how the reward for going forward and backward change with the state. Also, try to tune the gamma parameter to see how this may influence the expected reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episode = 1\n",
    "\n",
    "qTable.eval(env, total_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the continous and infinite environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) CartPole\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "Have a look at this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"252.045544pt\" version=\"1.1\" viewBox=\"0 0 366.6475 252.045544\" width=\"366.6475pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-01-16T15:53:17.246930</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 252.045544 \r\nL 366.6475 252.045544 \r\nL 366.6475 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 33.2875 228.167419 \r\nL 359.4475 228.167419 \r\nL 359.4475 10.727419 \r\nL 33.2875 10.727419 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p272ad4de86)\">\r\n    <image height=\"218\" id=\"image65ffc856f1\" transform=\"scale(1 -1)translate(0 -218)\" width=\"327\" x=\"33\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAAUcAAADaCAYAAAA1xoBCAAAFFklEQVR4nO3cTWtcZRjH4fs007w0rTYNrW1JoWJbQlCCLvQb6NJ+A/fuCy7d5Du476JQFwZXWuzelRJQKAQr1YUxWIg6eTFpe1wU+pa/mEU6Z0iua/kww3MvDj/mnDk8Tdu2bQHwnCNdDwAwjMQRIBBHgEAcAQJxBAjEESAQR4BAHAECcQQIxBEgEEeAQBwBAnEECMQRIBBHgEAcAQJxBAjEESAQR4BAHAECcQQIxBEgEEeAQBwBAnEECMQRIBBHgEAcAQJxBAjEESAQR4BAHAECcQQIxBEgEEeAQBwBAnEECMQRIBBHgEAcAQJxBAjEESAQR4BAHAECcQQIel0PwMF18+bNunHjxkD2mpubq4WFhYHsxeEgjrw0d+7cqcXFxYHsdf/+/YHsw+HhthogEEeAQBwBAnEECMQRIOitrKx0PQMHVL/fH9he29vb5VpmP/WuX7/e9QwcUEtLSwPba3V1tVzL7KfetWvXup6BA2pzc7Nu3bq1p8++fflsXTw7VW1bdfu7n6u/+U9VtdW2e9trZmamXMvsJy+BMxQ+eHe2Zuc/qtWtmbr4zt/Vqz/r9M6X9clnX+w5kLCfxJGh8FN/vsbW36yqpl6dmnq8uPFLVS1WlToyeP6tZihsPjxeVc1za/fW57oZBkocGRK9Zrte/IV4bvxuN8NAiSND4vKJ7+vM2K810uzUSLNTkyNrNXNsudxS0xXPHOnc8YnRunJ+ot6a+qYetkerqqppHtVX3y5rI50RR16a2dnZunr16v9+bnpypOYvnaojzYPq1YMn61tjZ+vDPXy/6vF5jrCfmrb1ogTd2lpbqR8//7RefGfnjfc/rpMX57sZikPPM0eAQBwBAnEECMQRIBBHgEAcAQJxBAjEESAQR4BAHAECcQQIxBEgEEc6t7Px166jyY70RmtkdLybgaDEkSGw+sPterGOE9MX6vi5K90MBCWODIH/OjWvaZq4DoMgjgCBOAIE4ggQiCNAII4AgTgCBOIIEIgjQCCOAIE4AgTiCBCII0AgjgCBONKpB1v92llf27V+bHpm8MPAM8SRTm2t/V4bf9zbtX7q0nsdTANPiSNAII4AgTgCBOIIEIgjQCCOAIE4AgTiCBCII0AgjgCBOAIE4ggQiCNAII506uH2RtcjQCSOdGpl6etda+NT52v0xHQH08BT4kin2kePdq2NvXK6RidPDn4YeIY4AgTiCBCII0AgjgCBOAIE4ggQiCNAII4AgTgCBOIIEIgjQCCOAIE4AgS9rgfgcJs883qNHB2rqqq2bav/23KNn3yt46mgqmnbtu16CKh6fHzZ+urdmpi+8CSY0BVxBAg8cwQIxBEgEEeAQBwBAnEECMQRIBBHgEAcAQJxBAjEESAQR4BAHAECcQQIxBEgEEeAQBwBAnEECMQRIBBHgEAcAQJxBAjEESAQR4BAHAECcQQIxBEgEEeAQBwBAnEECMQRIBBHgEAcAQJxBAjEESAQR4BAHAECcQQIxBEgEEeAQBwBAnEECMQRIBBHgEAcAQJxBAjEESAQR4BAHAECcQQIxBEgEEeAQBwBAnEECMQRIBBHgEAcAQJxBAjEESAQR4BAHAECcQQIxBEgEEeAQBwBAnEECMQRIBBHgEAcAQJxBAjEESAQR4BAHAECcQQIxBEgEEeAQBwBAnEECMQRIBBHgEAcAQJxBAjEESAQR4BAHAECcQQIxBEgEEeAQBwBAnEECMQRIBBHgEAcAYJ/AbmrqL66zN6oAAAAAElFTkSuQmCC\" y=\"-10.045544\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"me395f51fa5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.5593\" xlink:href=\"#me395f51fa5\" y=\"228.167419\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(30.37805 242.765856)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"87.9193\" xlink:href=\"#me395f51fa5\" y=\"228.167419\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(78.37555 242.765856)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"142.2793\" xlink:href=\"#me395f51fa5\" y=\"228.167419\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(132.73555 242.765856)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"196.6393\" xlink:href=\"#me395f51fa5\" y=\"228.167419\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 300 -->\r\n      <g transform=\"translate(187.09555 242.765856)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"250.9993\" xlink:href=\"#me395f51fa5\" y=\"228.167419\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 400 -->\r\n      <g transform=\"translate(241.45555 242.765856)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"305.3593\" xlink:href=\"#me395f51fa5\" y=\"228.167419\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 500 -->\r\n      <g transform=\"translate(295.81555 242.765856)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mef23acb973\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mef23acb973\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(19.925 14.798438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mef23acb973\" y=\"38.179219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(13.5625 41.978438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mef23acb973\" y=\"65.359219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(7.2 69.158438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mef23acb973\" y=\"92.539219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 150 -->\r\n      <g transform=\"translate(7.2 96.338437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mef23acb973\" y=\"119.719219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(7.2 123.518438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mef23acb973\" y=\"146.899219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 250 -->\r\n      <g transform=\"translate(7.2 150.698438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mef23acb973\" y=\"174.079219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 300 -->\r\n      <g transform=\"translate(7.2 177.878437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mef23acb973\" y=\"201.259219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 350 -->\r\n      <g transform=\"translate(7.2 205.058437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 33.2875 228.167419 \r\nL 33.2875 10.727419 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 359.4475 228.167419 \r\nL 359.4475 10.727419 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 33.2875 228.167419 \r\nL 359.4475 228.167419 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 33.2875 10.727419 \r\nL 359.4475 10.727419 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p272ad4de86\">\r\n   <rect height=\"217.44\" width=\"326.16\" x=\"33.2875\" y=\"10.727419\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-8e6dc5d3918e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# take a random action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m             \u001b[0mformat_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m                 \u001b[1;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36mformat\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m                 \u001b[1;31m# FIXME: log the exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-2>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[1;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    339\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m             \u001b[1;31m# Finally look for special method names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(fig)\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[0mjpg_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'jpg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'svg'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m         \u001b[0msvg_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'svg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'pdf'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mpdf_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pdf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'svg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2191\u001b[0m                            else suppress())\n\u001b[0;32m   2192\u001b[0m                     \u001b[1;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2193\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2195\u001b[0m                     bbox_inches = self.figure.get_tightbbox(\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   1861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1863\u001b[1;33m             mimage._draw_list_compositing_images(\n\u001b[0m\u001b[0;32m   1864\u001b[0m                 renderer, self, artists, self.suppressComposite)\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;31m# Composite any adjacent images together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m                          \u001b[1;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m                 **kwargs)\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self, renderer, inframe)\u001b[0m\n\u001b[0;32m   2745\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2747\u001b[1;33m         \u001b[0mmimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0martists\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2749\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'axes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 \u001b[0mimage_group\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m                 \u001b[0mflush_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m                 \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mflush_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mflush_images\u001b[1;34m()\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0mimage_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_group\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                 \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomposite_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_group\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                     \u001b[0mgc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_gc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mcomposite_images\u001b[1;34m(images, renderer, magnification)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mbboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagnification\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mmagnification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mmake_image\u001b[1;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[0;32m    924\u001b[0m         clip = ((self.get_clip_box() or self.axes.bbox) if self.get_clip_on()\n\u001b[0;32m    925\u001b[0m                 else self.figure.bbox)\n\u001b[1;32m--> 926\u001b[1;33m         return self._make_image(self._A, bbox, transformed_bbox, clip,\n\u001b[0m\u001b[0;32m    927\u001b[0m                                 magnification, unsampled=unsampled)\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36m_make_image\u001b[1;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[0;32m    548\u001b[0m                     \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_rgb_to_rgba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m                 \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_scalar_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m                 output_alpha = _resample(  # resample alpha channel\n\u001b[0m\u001b[0;32m    551\u001b[0m                     self, A[..., 3], out_shape, t, alpha=alpha)\n\u001b[0;32m    552\u001b[0m                 output = _resample(  # resample rgb channels\n",
      "\u001b[1;32mF:\\python385\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36m_resample\u001b[1;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresample\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[0mresample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m     _image.resample(data, out, transform,\n\u001b[0m\u001b[0;32m    193\u001b[0m                     \u001b[0m_interpd_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    display(plt.gcf())    \n",
    "    clear_output(wait=True)\n",
    "    new_state, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, have a sense of how a random approach works for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The average reward is 22.3222\n"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "total_reward = 0\n",
    "total_episode = 10000\n",
    "for episode in range(total_episode):\n",
    "    env.reset()\n",
    "    while True:\n",
    "        new_state, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()\n",
    "print(\"The average reward is {}\".format(total_reward / total_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is continous task. For this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Though we can somehow discretize the state and apply the *qtable* approach, we'll replace it with a approximator that will approximate the *qtable* lookup function. Thus we can no longer apply the *qtable* for this task. \n",
    "\n",
    "As a result, our Q value, $Q(s, a)$ is calculated by passing in a state to the approximator. The output will be Q values for each available action.\n",
    "\n",
    "We can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$, which is exactly a regression problem.\n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ approximator to run the optimizer and update the weights.\n",
    "\n",
    "Note that since this is a regression task, we can use any regressor we have learned before. At first, we will try to use linear regression for this task. Feel free to use TensorFlow or Pytorch or even the code you have written in previous homework. \n",
    "\n",
    "FYI: GPU should provide more efficient training for these models, though a modern CPU should be enough for this homework. Our laptop (i5) can finish all the training tasks in dozens of minutes.\n",
    "\n",
    "### TODO: implement the Approximator class in *approximator.py* with linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful readings:\n",
    "* https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "* https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n"
    },
    {
     "ename": "NameError",
     "evalue": "name 'Approximator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7d3582e80dce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# begin answer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# end answer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m model = Approximator(state_size, action_size,\n\u001b[0m\u001b[0;32m     20\u001b[0m                      learning_rate=learning_rate, memory_pool_size=10000, batch_size=20)\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Approximator' is not defined"
     ]
    }
   ],
   "source": [
    "# If you like tensorflow\n",
    "# from approximator_tf import Approximator\n",
    "# If you like pytorch (Recommend)\n",
    "from approximator_torch import Approximator\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "gamma = 0.95 # discounting rate\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape[0]\n",
    "# tune the learning_rate and total_episode yourself\n",
    "# remember how learning_rate can influence the training for NNs?\n",
    "learning_rate = 0.01\n",
    "total_episode = 10\n",
    "# begin answer\n",
    "# end answer\n",
    "model = Approximator(state_size, action_size,\n",
    "                     learning_rate=learning_rate, memory_pool_size=10000, batch_size=20)\n",
    "\n",
    "model.train(env, total_episode)\n",
    "# plot to see the training rewards of each episode\n",
    "plt.plot(np.arange(len(model.reward_list)), np.array(model.reward_list))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how linear approximator struggles to learn a good q-function for this task. The reason might be that the Q function for this problem should be a highly non-linear function; thus a linear approximator cannot approximate it well. We should use a more complex approximator. **In many cases, when you want to use a complex approximator, you should consider about neural networks.**\n",
    "\n",
    "Now we will try to use neural networks as approximator. Feel free to use TensorFlow or Pytorch or even the code you have written in **Assignment3** to implement. Update your Approximator class to use a neural network as the approximator, and rerun the cell above.\n",
    "\n",
    "Use a simple network should be enough for this a task. An example can be:\n",
    "\n",
    "State->FC->Relu->FC->Relu->FC->QValue\n",
    "\n",
    "You can decide the number of neurons yourself, but 64 should be enough.\n",
    "\n",
    "### TODO: implement a neural network in your Approximator class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "\n",
    "Disappointedly, you may find that the approximator still struggle to converge and get good results. There are several potential reasons for this phenomenon. One reason is that the *new_state* is generated from current *state*, which violates the i.i.d assumption in supervised learning. And this may easily lead to bad convergence performance. One popular strategy for improving the convergence rate and making data more i.i.d is **experience replay**.\n",
    "\n",
    "Basically, when using **experience replay**, we would like to keep a memory pool filled with the experience the agent had before. More specifically, we would save the *(state, action, reward, new_state, done)* tuple in a memory pool, which denotes the previous experience. For each step of training, we sample a batch of experiences from the memory pool, and train the approximator with this batch. This can be seen as replaying a batch of previous experiences. Also, we would generate a new experience according to the current state and the approximator, and add this new experience to the memory pool.\n",
    "\n",
    "Note that in this case, the experience an agent has can be used to train the approximator multiple times. In some real-world applications, the experience may be very expensive to gain, and thus experience replay enables a more efficient use of previous experience. Also, as approximator is trained with a batch, the SGD optimizer typically converges more fast.\n",
    "\n",
    "### TODO: implement experience replay in your Approximator class\n",
    "Now please complete the **experience_replay** function in your Approximator class, and retrain the approximator. This time, you should find that the approximator can get pretty reasonable results. Check the average reward for each episode below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reward_list = model.eval(env, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, it may be interesting to have a look at a demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    display(plt.gcf())    \n",
    "    clear_output(wait=True)\n",
    "    action = model.take_action(state)\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    state = new_state\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Double Q-Learning\n",
    "\n",
    "As we only use the same model to both determine the greedy policy and to determine its value, sometimes the model may learn unrealistically high action values because it includes a maximization step over estimated action values, which tends to prefer over-estimated to underestimated values. Imagine that the model learns an over-estimated value for one action, then this action is very more likely to be selected. Further, as the values are estimated with the same model, the updated value for this action is still over-estimated, which makes the other actions difficult to be explored and updated. As a result, the convergence may be slowed down, and even the performance of the final model may be harmed.\n",
    "\n",
    "As can be seen, the over-estimated problem is partly caused by using the same model to determine the policy and value. Then how can we address this problem? A natural idea is to decouple these two steps by using two different models to determine the policy and value respectively. This is called **double Q-learning**. We call the model that determine the policy as Policy Model and the model that determine the value as Value Model. To avoid introduce an additional network, a very smart trick is like this: when training the models with one episode, we freeze the Value Model and update only the Policy Model. After one episode is trained, we update the Value Model with the weights of the Policy Model. Alternatively, we can update the Value Model with the weights of Policy Model periodically (say, 500 iterations). Although the Policy Model and the Value Model are not fully decoupled in this way, this approach is simple to implement and avoid further network designs. As you will see, this works well in practice.\n",
    "\n",
    "### TODO: implement double Q-learning in your Approximator class\n",
    "\n",
    "After implementing the double Q-learning, train another approximator and compare the performance with previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# tune the learning_rate yourself\n",
    "# remember how learning_rate can influence the training for NNs?\n",
    "learning_rate = 0.01\n",
    "# begin answer\n",
    "# end answer\n",
    "model = Approximator(state_size, action_size,\n",
    "                     learning_rate=learning_rate,\n",
    "                     memory_pool_size=10000,\n",
    "                     batch_size=20,\n",
    "                     double_QLearning=True)\n",
    "\n",
    "model.train(env, total_episode)\n",
    "# plot to see the training rewards of each episode\n",
    "plt.plot(np.arange(len(model.reward_list)), np.array(model.reward_list))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the average rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reward_list = model.eval(env, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    display(plt.gcf())    \n",
    "    clear_output(wait=True)\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    action = model.take_action(state)\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    state = new_state\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have finished the journey of reinforcement learning here. There are also some other environments available in *gym* for you to play with. Equipped with the techniques you have learned so far, just feel free to play with them if interested.\n",
    "\n",
    "Actually, you have learned many practical techniques for reinforcement learning. Theortically, you can even build some famous systems such as AlphaGo mainly based on these techniques :) In AlphaGo, Atari, and many other real-world applications, the agents need to deal with the image/video input. Thus it is naturally to use CNNs as approximators. There are also some other techniques used in these systems, such as Monte Carlo Tree Search in AlphaGo, but the big pictures are similar.\n",
    "\n",
    "To apply reinforcement learning in real-world problems, there are many other difficulties. I think the biggest challenge may be how to formulate your problems. What are the states? What are the rewards? Are they reasonable enough? How to gain experience? Unlike the environment in this notebook, in real world, these problems are often undefined. You need to formulate them yourself. Another challenge may be that the hyperparameters tuning would be quite difficult, even more difficult than CNNs/RNNs for some typical supervised/unsupervised learning problems. It requires some experience and practice :)\n",
    "\n",
    "This may be the last assignment for this course. Thank you for participating our course and completing our assignments :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}